{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aad527b-575e-40c6-9cd2-ebf90bc626d8",
   "metadata": {},
   "source": [
    "# Deep dive into LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae972724-f345-474a-9f09-ad04b4391e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./files/requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ed0503-0ed8-4db9-a3cc-130173d79bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.97.1\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a698b-01a1-4800-8b04-5143067c9da1",
   "metadata": {},
   "source": [
    "## Python dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea28763-3459-4da3-811c-61125fa3b502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1337'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ.get('PORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa229cb5-83fb-4a8e-a56c-67654817ad6b",
   "metadata": {},
   "source": [
    "## Chat Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b67935-4bf8-4f23-9cb5-ad7c7aade285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a decentralized platform that uses blockchain technology to provide accurate and secure language translation services.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "output = llm.invoke('Explain LangChain in one sentence.')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d59d122-4357-4058-92b3-93cffa1f3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69426de3-b00b-4485-9a8c-d9cd285f57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! LangChain is a powerful tool used in natural language processing (NLP) tasks like text classification, sentiment analysis, and chatbots. It provides pre-trained models and tools to help developers build and deploy NLP applications quickly and easily.\n",
      "\n",
      "With LangChain, beginners can easily create NLP models without needing to start from scratch. The tool offers a variety of functionalities such as tokenization, part-of-speech tagging, named entity recognition, and more. By using LangChain, developers can expedite the development process and focus more on customizing the models to fit their specific needs.\n",
      "\n",
      "In essence, LangChain simplifies the development of NLP applications by providing ready-to-use tools and models, enabling beginners to create sophisticated language processing applications with ease.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content='You are a professional AI engineer with many years of experience'),\n",
    "    HumanMessage(content='Explain the LangChain tool in a clear and simple way to a beginner in the field.')\n",
    "]\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ec262-d00b-4c17-8bfe-084df693e81e",
   "metadata": {},
   "source": [
    "## Caching LLM Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3652d-5392-4ecd-b804-8ac34591f238",
   "metadata": {},
   "source": [
    "### 1. In-Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7fc4d67-a6ed-42c2-bf60-fa6704a7e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07046716-a36b-4d57-a4dc-21a76915b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 ms, sys: 3.45 ms, total: 18.7 ms\n",
      "Wall time: 1.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing! '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# First request (not in cache, takes longer)\n",
    "prompt = 'Tell me a joke a toddler can understand'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43855848-90dd-42f4-9cc2-e1e8c932fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 425 μs, sys: 1e+03 ns, total: 426 μs\n",
      "Wall time: 440 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the tomato turn red?\\n\\nBecause it saw the salad dressing! '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516a0b8-be52-47ec-91d5-f0b625720cfe",
   "metadata": {},
   "source": [
    "### 2. SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1987f36b-f5f0-4d4e-a626-f7bdf579f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.57 ms, sys: 4.28 ms, total: 9.85 ms\n",
      "Wall time: 7.35 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy was the math book sad? Because it had too many problems.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path='.langchain.db'))\n",
    "\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke('Tell me a joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86085e7b-b3c6-4ec8-bf53-31dabac84b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.95 ms, sys: 1.31 ms, total: 4.27 ms\n",
      "Wall time: 3.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy was the math book sad? Because it had too many problems.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke('Tell me a joke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b32b5-35dd-43af-b820-ae24ecc453e3",
   "metadata": {},
   "source": [
    "## LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885c754a-13ca-430f-9d00-5ba307675185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Beneath the pale moonlight,\n",
      "A raven takes flight,\n",
      "Silhouetted against the stars,\n",
      "Its caw echoing from afar.\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, forever intertwined,\n",
      "In the darkness, they both shine,\n",
      "Guiding us through the night,\n",
      "With their eerie, haunting light.\n",
      "\n",
      "Verse 2:\n",
      "The Moon, a silent observer,\n",
      "Watching over us, forever,\n",
      "While the raven soars high,\n",
      "A messenger from the sky.\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, forever intertwined,\n",
      "In the darkness, they both shine,\n",
      "Guiding us through the night,\n",
      "With their eerie, haunting light.\n",
      "\n",
      "Bridge:\n",
      "With wings outstretched, the raven calls,\n",
      "A symbol of mystery that enthralls,\n",
      "While the Moon silently keeps watch,\n",
      "A celestial guardian, never to botch.\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, forever intertwined,\n",
      "In the darkness, they both shine,\n",
      "Guiding us through the night,\n",
      "With their eerie, haunting light.\n",
      "\n",
      "Outro:\n",
      "So let the Moon and raven guide your way,\n",
      "Through the darkness, they will stay,\n",
      "A symbol of strength and grace,\n",
      "In this chaotic, bewildering place.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea0c76c5-a24f-4dd0-99fd-cc802efd2944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "In the dead of night, in the pale moonlight\n",
      "A raven swoops down, a shadow in flight\n",
      "Its eyes are black, its feathers shine\n",
      "A mysterious creature, of ominous design\n",
      "\n",
      "Chorus:\n",
      "Oh, Moon and Raven, dance in the sky\n",
      "An eternal bond, never to die\n",
      "Their spirits intertwined, forever entwined\n",
      "In the darkness of the night, their secrets they'll find\n",
      "\n",
      "Verse 2:\n",
      "The Moon whispers secrets, to the Raven's ear\n",
      "Of ancient tales of love and fear\n",
      "They soar through the night, in a symphony of sound\n",
      "Their connection unbreakable, forever bound\n",
      "\n",
      "Chorus:\n",
      "Oh, Moon and Raven, dance in the sky\n",
      "An eternal bond, never to die\n",
      "Their spirits intertwined, forever entwined\n",
      "In the darkness of the night, their secrets they'll find\n",
      "\n",
      "Bridge:\n",
      "As the Raven cries out, to the silvery Moon\n",
      "Their connection grows stronger, in the quiet of the gloom\n",
      "They are creatures of the night, with stories untold\n",
      "A bond that will never break, a tale to behold\n",
      "\n",
      "Chorus:\n",
      "Oh, Moon and Raven, dance in the sky\n",
      "An eternal bond, never to die\n",
      "Their spirits intertwined, forever entwined\n",
      "In the darkness of the night, their secrets they'll find\n",
      "\n",
      "Outro:\n",
      "So when you see the Moon, shining bright\n",
      "And hear the Raven's cry in the dead of night\n",
      "Remember their bond, forever strong\n",
      "A timeless connection, a haunting song."
     ]
    }
   ],
   "source": [
    "# Enable streaming\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2df6a-7c2a-490b-bf48-1a9b9a9ffc8d",
   "metadata": {},
   "source": [
    "## Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d0f5b-d694-494b-87de-4f9ae5583113",
   "metadata": {},
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd01296-a24d-4c56-b599-afbbcab823fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an experienced hiring manager. \\nWrite a few sentences on what you look out for in a candidate's resume and headline on linkedin\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = '''You are an experienced hiring manager. \n",
    "Write a few sentences on what you look out for in a candidate's {tool1} and {tool2} on linkedin\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "prompt = prompt_template.format(tool1='resume', tool2='headline')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec5b9ca-2100-48f2-b91e-f32291ba9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When reviewing a candidate's resume, I look for a clear and concise summary of their experience, skills, and accomplishments. I also pay attention to the layout and formatting, as well as any relevant certifications or education. On LinkedIn, I look for a professional headline that accurately reflects the candidate's expertise and career goals. I also consider the quality of their profile picture and the content they share on their profile. Overall, I value candidates who present themselves in a professional and engaging manner.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26105d-1fd6-4977-b786-2dafbfbdfb62",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71b97ad-d8a4-4c6f-8a96-8c2918c48423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in JSON format', additional_kwargs={}, response_metadata={}), HumanMessage(content='Top 10 countries in World by population', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "#from langchain.prompts import SystemMessagePromptTemplate # For dynamic content in system message\n",
    "from langchain_core.messages import SystemMessage # When dynamic content is not needed\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in JSON format'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n='10', area='World')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be75c4f-27ec-4bb2-90e7-64b6b2183466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": \"China\",\n",
      "    \"2\": \"India\",\n",
      "    \"3\": \"United States\",\n",
      "    \"4\": \"Indonesia\",\n",
      "    \"5\": \"Pakistan\",\n",
      "    \"6\": \"Brazil\",\n",
      "    \"7\": \"Nigeria\",\n",
      "    \"8\": \"Bangladesh\",\n",
      "    \"9\": \"Russia\",\n",
      "    \"10\": \"Mexico\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f752de-00f5-4743-b6cc-9ede267c2f00",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82487240-b242-486b-96f8-2a264870be5a",
   "metadata": {},
   "source": [
    "### Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb9766c3-292e-4215-a402-43c5421f417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIV, or human immunodeficiency virus, is a retrovirus that attacks the immune system, specifically targeting CD4 cells which are crucial for the body's ability to fight off infections. When left untreated, HIV can progress to acquired immunodeficiency syndrome (AIDS), a condition in which the immune system is severely weakened, leaving individuals susceptible to opportunistic infections and certain cancers. HIV is primarily transmitted through sexual contact, sharing needles, and from mother to child during childbirth or breastfeeding. Advances in antiretroviral therapy have greatly improved the prognosis for individuals living with HIV, turning what was once a fatal disease into a manageable chronic condition.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "output = chain.invoke({'virus': 'HIV', 'language': 'English'})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e330aa7-046d-4830-8132-ccba45fc2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/tsds32ps4619kckrn0jhpyjh0000gq/T/ipykernel_24544/2046776619.py:5: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Country:  Nigeria\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of Nigeria?. List the top 3 places to visit in that city. Use bullet points\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of Nigeria is Abuja.\n",
      "\n",
      "Top 3 places to visit in Abuja:\n",
      "- Zuma Rock\n",
      "- Aso Rock\n",
      "- National Mosque\n"
     ]
    }
   ],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points'\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# the pattern used above is the latest pattern, this one has been deprecated but I want to see the 'Verbose' in action\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# the latest pattern to be used\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "country = input('Enter Country: ')\n",
    "output = chain.invoke(country)\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df29327-01f2-4956-8e2e-5d4edbf80479",
   "metadata": {},
   "source": [
    "### Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb0fabce-66cd-450d-a14f-6b70951f8393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is a simple implementation of linear regression in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    # Add a column of ones to X for the intercept term\n",
      "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
      "    \n",
      "    # Calculate the coefficients using the normal equation\n",
      "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
      "    \n",
      "    return theta\n",
      "\n",
      "# Example usage\n",
      "X = np.array([[1], [2], [3], [4]])\n",
      "y = np.array([2, 4, 6, 8])\n",
      "\n",
      "theta = linear_regression(X, y)\n",
      "print(theta)\n",
      "```\n",
      "\n",
      "This function takes in the feature matrix `X` and the target vector `y`, adds a column of ones to `X` for the intercept term, and then calculates the coefficients `theta` using the normal equation. Finally, it returns the coefficients `theta` that define the linear regression model.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThis Python code implements a simple linear regression model. Here's a detailed explanation of the code:\n",
      "\n",
      "1. First, the necessary library `numpy` is imported to handle numerical operations efficiently.\n",
      "\n",
      "2. The `linear_regression` function is defined, which takes two parameters `X` and `y` representing the feature matrix (input data) and target vector (output data), respectively.\n",
      "\n",
      "3. Inside the function, a column of ones is added to the feature matrix `X`. This is done to account for the intercept term in the linear regression model.\n",
      "\n",
      "4. The coefficients `theta` are calculated using the normal equation:\n",
      "\n",
      "    \\[\n",
      "    theta = (X^T X)^{-1} X^T y\n",
      "    \\]\n",
      "\n",
      "    Without going into too much detail, this equation provides a closed-form solution to finding the optimal values of the coefficients that minimize the mean squared error of the linear regression model.\n",
      "\n",
      "5. The calculated coefficients `theta` are returned from the function.\n",
      "\n",
      "6. An example usage of the function is shown by creating a simple `X` array with values 1 to 4 and `y` representing a linear relationship. The `linear_regression` function is called, and the resulting coefficients `theta` are printed to the console.\n",
      "\n",
      "In summary, the `linear_regression` function efficiently calculates the coefficients for a linear regression model using the normal equation and provides a simple example of its usage with dummy data.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.invoke('Linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edb07f0b-fa07-4b09-b79d-a6cf41e5cc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Python code implements a simple linear regression model. Here's a detailed explanation of the code:\n",
      "\n",
      "1. First, the necessary library `numpy` is imported to handle numerical operations efficiently.\n",
      "\n",
      "2. The `linear_regression` function is defined, which takes two parameters `X` and `y` representing the feature matrix (input data) and target vector (output data), respectively.\n",
      "\n",
      "3. Inside the function, a column of ones is added to the feature matrix `X`. This is done to account for the intercept term in the linear regression model.\n",
      "\n",
      "4. The coefficients `theta` are calculated using the normal equation:\n",
      "\n",
      "    \\[\n",
      "    theta = (X^T X)^{-1} X^T y\n",
      "    \\]\n",
      "\n",
      "    Without going into too much detail, this equation provides a closed-form solution to finding the optimal values of the coefficients that minimize the mean squared error of the linear regression model.\n",
      "\n",
      "5. The calculated coefficients `theta` are returned from the function.\n",
      "\n",
      "6. An example usage of the function is shown by creating a simple `X` array with values 1 to 4 and `y` representing a linear relationship. The `linear_regression` function is called, and the resulting coefficients `theta` are printed to the console.\n",
      "\n",
      "In summary, the `linear_regression` function efficiently calculates the coefficients for a linear regression model using the normal equation and provides a simple example of its usage with dummy data.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05ce48-2635-4a30-afaf-57a4f6219119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
