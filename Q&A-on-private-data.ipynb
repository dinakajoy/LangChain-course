{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df54c5d-b32a-44a5-a808-52d930968e72",
   "metadata": {},
   "source": [
    "# Project: Question-Answering on Private Documents Using LangChain, OpenAI and Pinecone - RAG\n",
    "\n",
    "- Load Documents\n",
    "- Chunk Data\n",
    "- Calculate Cost\n",
    "- Using Pinecone as a vector DB\n",
    "- Using Chroma as a vector DB\n",
    "- Asking and Getting Answers\n",
    "  - Run the functions\n",
    "- Adding Memory (Chat History)\n",
    "- Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06a36f8-cc1e-4e0c-926e-09367b77bc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168d2ad2-b263-4752-aed8-0ae1f7c6dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d62d0a-a829-4081-b7d2-26a17e511db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ae7a77-cf39-4310-a616-031e0fcfc6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade -q openai langchain tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527443ae-111e-4b71-a5c9-13cc2c2b5b0f",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99526709-4825-4edc-bb88-fd8d3b042cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF, Text, Doc files\n",
    "def load_documents(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file)\n",
    "    else:\n",
    "        print('Document format not supported!')\n",
    "        return None\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea78faa-c8c0-4eeb-b1cc-87c1b46e7e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338efa22-fb26-445e-be0f-9520ba0bd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikepedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac113c-73c4-490e-97bd-b612a236d0c2",
   "metadata": {},
   "source": [
    "## Chunk Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "823795a8-c9a4-471c-ac7d-3420dc9e328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c35b8-6116-4db1-9cdb-9c4745d5a660",
   "metadata": {},
   "source": [
    "## Calculate Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e6112a-8157-4e1b-b550-90f2d837e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5bc36-205e-474a-a93f-71466453617e",
   "metadata": {},
   "source": [
    "## Using Pinecone as a vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d60bcb64-c176-4613-8de9-d87907a2b008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc8e2ad-bf1d-4a29-84bd-179f9a53685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete index(es)\n",
    "\n",
    "def delete_pinecode_index(index_name='all'):\n",
    "    from pinecone import Pinecone\n",
    "    pc = Pinecone()\n",
    "\n",
    "    if index_name == 'all':\n",
    "        print('Deleting all indexes...')\n",
    "        \n",
    "        indexes = pc.list_indexes().names()\n",
    "        for i in indexes:\n",
    "            pc.delete_index(i)\n",
    "        print('Done!')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name}...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfa7e587-4618-4203-88f1-0e8acd2eebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Uploading Embeddings to a vector database (Pinecone)\n",
    "\n",
    "def insert_or_fetch_embeddings_pinecone(index_name, chunks):\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "    \n",
    "    pc = Pinecone()\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        print(f'Creating index: {index_name} and embeddings...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "            # spec=PodSpec(environment='us-west1-gcp')\n",
    "        )\n",
    "        vector_store = PineconeVectorStore.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name\n",
    "        )\n",
    "        print('Index created!')\n",
    "    else:\n",
    "        print(f'Index {index_name} already exists! Loading embeddings...', end='')\n",
    "        vector_store = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "        print('Done!')\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa87079-1e8a-4044-a5ed-b1c7c728525b",
   "metadata": {},
   "source": [
    "## Using Chroma as a vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "623152f2-9560-44a1-b5d2-912487fa2dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain-chroma chromadbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fdb7b58-ddca-4cb4-ba03-dcaa1a0b1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d01d59-3459-4f18-8781-a0bb96946355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3f039-6325-4a80-8b04-5c206f86408b",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8904039f-22d6-4a23-b93b-bc9c37879e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "\n",
    "    answer = qa_chain.invoke(q)\n",
    "    return answer['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efab8a-3c4c-4065-9ed7-1f228bce97be",
   "metadata": {},
   "source": [
    "## Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6636580-f551-4f30-9e62-2bca5c1935a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./files/us_constitution.pdf\n",
      "There are 1500 characters on page 2\n",
      "You have 41 pages in your file\n"
     ]
    }
   ],
   "source": [
    "pdf_data = load_documents('./files/us_constitution.pdf')\n",
    "# print(pdf_data[1].page_content)\n",
    "# print(pdf_data[1].metadata)\n",
    "\n",
    "print(f'There are {len(pdf_data[1].page_content)} characters on page 2')\n",
    "print(f'You have {len(pdf_data)} pages in your file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a761f498-a95d-443a-b67f-25bf85144457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./files/the_great_gatsby.docx\n",
      "There are 296672 characters on page 1\n",
      "You have 1 page in your file\n"
     ]
    }
   ],
   "source": [
    "docs_data = load_documents('./files/the_great_gatsby.docx')\n",
    "# print(docs_data[0].page_content)\n",
    "# print(docs_data[0].metadata)\n",
    "\n",
    "print(f'There are {len(docs_data[0].page_content)} characters on page 1')\n",
    "print(f'You have {len(docs_data)} page in your file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6202f4cf-5528-4ed6-8837-a8cd6a9135c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21271 characters on page 1\n",
      "You have 1 page in your file\n"
     ]
    }
   ],
   "source": [
    "txt_data = load_documents('./files/churchill_speech.txt')\n",
    "\n",
    "print(f'There are {len(txt_data[0].page_content)} characters on page 1')\n",
    "print(f'You have {len(txt_data)} page in your file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f4a0c47-c07f-4c35-a483-4d8395e2ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a large language model developed by OpenAI and the fourth in its series of GPT foundation models.  \n",
      "GPT-4 is more capable than its predecessor GPT-3.5. GPT-4 Vision (GPT-4V) is a version of GPT-4 that can process images in addition to text. OpenAI has not revealed technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "An early version of GPT-4 was integrated by Microsoft into Bing Chat, launched in February 2023. GPT-4 was released in ChatGPT in March 2023, and removed in 2025. GPT-4 is still available in OpenAI's API.\n",
      "GPT-4, as a generative pre-trained transformer (GPT), was first trained to predict the next token for a large amount of text (both public data and \"data licensed from third-party providers\"). Then, it was fine-tuned for human alignment and policy compliance, notably with reinforcement learning from human feedback (RLHF).\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "\n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training\", which was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with over 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,048 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.\n",
      "To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.\n",
      "When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.\n",
      "A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his p\n"
     ]
    }
   ],
   "source": [
    "wiki_data = load_from_wikipedia('GPT-4')\n",
    "print(wiki_data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47acc07d-4d1a-47cf-8f53-e75db68723da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "Maryland six, V irginia ten, North Carolina five, South Carolina five, and \n",
      " Georgia three. \n",
      " When vacancies happen in the Representation from any State, the \n",
      " Executive Authority thereof shall issue W rits of Election to fill such \n",
      " V acancies.\n"
     ]
    }
   ],
   "source": [
    "pdf_chunks = chunk_data(pdf_data)\n",
    "print(len(pdf_chunks))\n",
    "print(pdf_chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8311f1-0a72-457c-8879-cd3fbd3a2c9f",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "973cbbac-5d53-4834-9c13-4143f3873ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "delete_pinecode_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff0eb327-9b0f-42a0-b51b-b19aa58166d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./files/us_constitution.pdf\n",
      "Creating index: askadocument and embeddings...Index created!\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "pdf_data = load_documents('./files/us_constitution.pdf')\n",
    "pdf_data_chunks = chunk_data(pdf_data, chunk_size=256)\n",
    "pinecone_vector_store = insert_or_fetch_embeddings_pinecone(index_name, pdf_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82509ee9-43fb-482d-8ac0-d20ef8f5e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but you haven't provided any specific document or context for me to determine what it is about. Could you please provide more details or context so I can assist you better?\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the document about?'\n",
    "answer = ask_and_get_answer(pinecone_vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c4f4efb-4836-4ef9-8277-00f0d5fc2374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write \"Quit\" or \"Exit\" to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  What is the document about?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The document provided is the United States Constitution. It outlines the framework for the government of the United States, including the establishment of justice, defense, welfare, and the supremacy of the Constitution and federal laws.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #2:  What is the first amendment described in the document?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The First Amendment described in the document states: \"Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof; or abridging the freedom of speech, or of the press; or the right of the people peaceably to assemble, and to petition the Government for a redress of grievances.\"\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #3:  what about the second amendment?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The Second Amendment to the United States Constitution states: \"A well regulated Militia, being necessary to the security of a free State, the right of the people to keep and bear Arms, shall not be infringed.\" It protects the right of individuals to keep and bear arms.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #4:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Goodbye! If you have any more questions in the future, feel free to ask.\n",
      "\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #5:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting... bye!\n"
     ]
    }
   ],
   "source": [
    "# Take questions for user (loop)\n",
    "\n",
    "import time\n",
    "i = 1\n",
    "print('Write \"Quit\" or \"Exit\" to quit')\n",
    "\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Exiting... bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answer(pinecone_vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n{\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384cd04-7590-45f2-8be7-9e44ed41f5ea",
   "metadata": {},
   "source": [
    "### Test creating and inserting vectors from Wikipedia data - Using Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a137c205-5a90-4dda-a91e-7a90557986fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "delete_pinecode_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d07e393-2fc8-4069-bee4-64da188dd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index: chatgpt-wiki and embeddings...Index created!\n"
     ]
    }
   ],
   "source": [
    "wiki_data = load_from_wikipedia('ChatGPT')\n",
    "wiki_data_chunks = chunk_data(wiki_data)\n",
    "index_name = 'chatgpt-wiki'\n",
    "pinecone_wiki_vector_store = insert_or_fetch_embeddings_pinecone(index_name, wiki_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caeca59e-712a-41c3-bf84-47eeb46a3229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT is a conversational AI model developed by OpenAI. It is based on the GPT-3 (Generative Pre-trained Transformer 3) architecture and is designed to generate human-like responses in natural language conversations. ChatGPT can be used in various applications such as chatbots, virtual assistants, and customer service interactions.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is chatGPT?'\n",
    "answer = ask_and_get_answer(pinecone_wiki_vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004616f-8e37-4d28-afce-4f61643fbf63",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e82ebf3-bc1e-40ea-bf22-1b997eb64ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_data = load_documents('./files/rag_powered_by_google_search.pdf')\n",
    "pdf_data_chunks = chunk_data(pdf_data, chunk_size=256)\n",
    "chroma_vector_store = create_embeddings_chroma(pdf_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53ab4e5e-d0be-4898-91c3-2211a34e6fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI Search is a feature within Google Cloud's Vertex AI platform that allows users to perform hybrid search capabilities. This feature enables users to search across structured and unstructured data sources, making it easier to find relevant information efficiently.\n"
     ]
    }
   ],
   "source": [
    "q = 'What is Vertex AI Search?'\n",
    "answer = ask_and_get_answer(chroma_vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3f512b6-9e48-40ec-9a2c-4afde2cb7e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The StackOverflow dataset had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "# Use loaded embeddings\n",
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "answer = ask_and_get_answer(db, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b556a54-3edb-4409-9cfd-4128146f8c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but it seems like your message got cut off. Could you please provide more context or clarify your question?\n"
     ]
    }
   ],
   "source": [
    "# Trying out memory\n",
    "q = 'Multiply that number by 2.'\n",
    "answer = ask_and_get_answer(chroma_vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd7bce-4df6-41ae-94ff-e37cbda1bc24",
   "metadata": {},
   "source": [
    "## Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b9a570a-569d-428b-9d32-c7cec9c21ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/tsds32ps4619kckrn0jhpyjh0000gq/T/ipykernel_12828/3633005402.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
    "retriever = chroma_vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a72b6f4-44fb-48ab-8737-3e15096184b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "114fb0a5-ddef-466d-8cec-c2e8df157b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./files/rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_documents('./files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd71292-2f93-4fb6-ab52-a8aed7955a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?', additional_kwargs={}, response_metadata={}), AIMessage(content='The dataset had 8 million pairs of questions and answers.', additional_kwargs={}, response_metadata={})], 'answer': 'The dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53b13025-556f-4c13-b200-5dc447f713fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ffdc89a-847d-43ba-86bc-c18706cce7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The dataset had 8 million pairs of questions and answers.\n",
      "Follow Up Input: Multiply that number by 2\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "Human: What is the result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 2?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'Multiply that number by 2', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?', additional_kwargs={}, response_metadata={}), AIMessage(content='The dataset had 8 million pairs of questions and answers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Multiply that number by 2', additional_kwargs={}, response_metadata={}), AIMessage(content='The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.', additional_kwargs={}, response_metadata={})], 'answer': 'The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'Multiply that number by 2'\n",
    "answer = ask_question(q, crc)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c44f5acb-f525-4986-811d-7e530afb4ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.\n"
     ]
    }
   ],
   "source": [
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e26f1d0b-65d3-443a-9b54-2b54f196d1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 2\n",
      "Assistant: The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.\n",
      "Follow Up Input: Divide the result by 200\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "e\u0000ciency of an advanced RAG system.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "e\u0000ciency of an advanced RAG system.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "e\u0000ciency of an advanced RAG system.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "e\u0000ciency of an advanced RAG system.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "e\u0000ciency of an advanced RAG system.\n",
      "Contact sales Get started for freeCloud Blog\n",
      "Human: What is the result of dividing 16 million by 200?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To find the result of dividing 16 million by 200, you can perform the calculation as follows:\n",
      "\n",
      "16,000,000 ÷ 200 = 80,000\n",
      "\n",
      "Therefore, the result is 80,000.\n"
     ]
    }
   ],
   "source": [
    "q = 'Divide the result by 200'\n",
    "answer = ask_question(q, crc)\n",
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b4ab20c-634f-49d4-babd-f7de00d0fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='How many pairs of questions and answers had the StackOverflow dataset?' additional_kwargs={} response_metadata={}\n",
      "content='The dataset had 8 million pairs of questions and answers.' additional_kwargs={} response_metadata={}\n",
      "content='Multiply that number by 2' additional_kwargs={} response_metadata={}\n",
      "content='The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.' additional_kwargs={} response_metadata={}\n",
      "content='Divide the result by 200' additional_kwargs={} response_metadata={}\n",
      "content='To find the result of dividing 16 million by 200, you can perform the calculation as follows:\\n\\n16,000,000 ÷ 200 = 80,000\\n\\nTherefore, the result is 80,000.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4cad0d-2cbb-404a-9ec6-493c7a5ea95d",
   "metadata": {},
   "source": [
    "### Loop for asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b3119ff-0912-4b27-9893-7120e2878e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  Tell me about Google Search technologies as described in the document.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 2\n",
      "Assistant: The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.\n",
      "Human: Divide the result by 200\n",
      "Assistant: To find the result of dividing 16 million by 200, you can perform the calculation as follows:\n",
      "\n",
      "16,000,000 ÷ 200 = 80,000\n",
      "\n",
      "Therefore, the result is 80,000.\n",
      "Follow Up Input: Tell me about Google Search technologies as described in the document.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "enabled search experiences for your public or internal websites, mobile\n",
      "applications, and various other enterprise search services. The product\n",
      "is the result of deep collaboration between the Google Search and\n",
      "\n",
      "enabled search experiences for your public or internal websites, mobile\n",
      "applications, and various other enterprise search services. The product\n",
      "is the result of deep collaboration between the Google Search and\n",
      "\n",
      "enabled search experiences for your public or internal websites, mobile\n",
      "applications, and various other enterprise search services. The product\n",
      "is the result of deep collaboration between the Google Search and\n",
      "\n",
      "enabled search experiences for your public or internal websites, mobile\n",
      "applications, and various other enterprise search services. The product\n",
      "is the result of deep collaboration between the Google Search and\n",
      "\n",
      "enabled search experiences for your public or internal websites, mobile\n",
      "applications, and various other enterprise search services. The product\n",
      "is the result of deep collaboration between the Google Search and\n",
      "Human: What does the document describe about Google Search technologies?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The document describes a product that enhances search experiences for public or internal websites, mobile applications, and various enterprise search services. This product is the outcome of a significant collaboration between the Google Search team and another unspecified entity or team. It emphasizes the integration and application of Google Search technologies to improve and customize search functionalities across different platforms and services.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:   Is Vertex AI Search a fully-managed platform?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 2\n",
      "Assistant: The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.\n",
      "Human: Divide the result by 200\n",
      "Assistant: To find the result of dividing 16 million by 200, you can perform the calculation as follows:\n",
      "\n",
      "16,000,000 ÷ 200 = 80,000\n",
      "\n",
      "Therefore, the result is 80,000.\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The document describes a product that enhances search experiences for public or internal websites, mobile applications, and various enterprise search services. This product is the outcome of a significant collaboration between the Google Search team and another unspecified entity or team. It emphasizes the integration and application of Google Search technologies to improve and customize search functionalities across different platforms and services.\n",
      "Follow Up Input:  Is Vertex AI Search a fully-managed platform?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "\n",
      "Related articles\n",
      "AI & Machine Learning\n",
      "Vertex AI Search adds new\n",
      "generative AI capabilities and\n",
      "enterprise-ready features\n",
      "Vertex AI search offers customizable answers,\n",
      "search tuning, vector search, grounding and\n",
      "compliance updates for enterprises.\n",
      "Human: Is Vertex AI Search considered a fully-managed platform?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, Vertex AI Search is considered a fully-managed platform. It provides a suite of tools and features designed to help enterprises implement search and AI capabilities without the need to manage the underlying infrastructure. This includes customizable answers, search tuning, vector search, grounding, and compliance updates, all of which are managed within the platform, making it easier for enterprises to integrate advanced search and AI functionalities into their applications.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  What advancements have been made in semantic search?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiply that number by 2\n",
      "Assistant: The result of multiplying the number of pairs of questions and answers in the dataset, which is 8 million, by 2 would be 16 million.\n",
      "Human: Divide the result by 200\n",
      "Assistant: To find the result of dividing 16 million by 200, you can perform the calculation as follows:\n",
      "\n",
      "16,000,000 ÷ 200 = 80,000\n",
      "\n",
      "Therefore, the result is 80,000.\n",
      "Human: Tell me about Google Search technologies as described in the document.\n",
      "Assistant: The document describes a product that enhances search experiences for public or internal websites, mobile applications, and various enterprise search services. This product is the outcome of a significant collaboration between the Google Search team and another unspecified entity or team. It emphasizes the integration and application of Google Search technologies to improve and customize search functionalities across different platforms and services.\n",
      "Human:  Is Vertex AI Search a fully-managed platform?\n",
      "Assistant: Yes, Vertex AI Search is considered a fully-managed platform. It provides a suite of tools and features designed to help enterprises implement search and AI capabilities without the need to manage the underlying infrastructure. This includes customizable answers, search tuning, vector search, grounding, and compliance updates, all of which are managed within the platform, making it easier for enterprises to integrate advanced search and AI functionalities into their applications.\n",
      "Follow Up Input: What advancements have been made in semantic search?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "\n",
      "Google Search sta\u0000ed incorporating semantic search in 2015, with the\n",
      "introduction of notewo\u0000hy AI search innovations like deep learning\n",
      "ranking system RankBrain. This innovation was quickly followed with\n",
      "Human: What are the recent advancements in semantic search technology?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "As of my last update in December 2023, there have been several advancements in semantic search technology, building upon the foundational work of systems like Google's RankBrain. While specific details about the very latest advancements might not be fully covered, the trends and technologies that have been shaping the field include:\n",
      "\n",
      "1. **Natural Language Processing (NLP) Improvements**: Advances in NLP, particularly with models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), have significantly enhanced the ability of search engines to understand the context and nuances of user queries. These models can process words in relation to all the other words in a sentence, rather than one-by-one in order, which helps in understanding the intent behind queries more accurately.\n",
      "\n",
      "2. **Knowledge Graphs and Entities**: Search engines have been expanding and refining their knowledge graphs, which are vast databases of information about entities (people, places, things) and their interrelations. This allows for more precise and informative answers to queries, as the search engine can draw upon a structured understanding of the world.\n",
      "\n",
      "3. **Neural Matching**: This involves the use of deep neural networks to understand the meanings of queries and the relevance of documents, even when the exact words of the query aren't present in the documents. This \"super-synonym\" system helps in matching the intent of the search with the most relevant content.\n",
      "\n",
      "4. **Personalization and Contextual Understanding**: There's an ongoing improvement in how search engines use contextual information (like location, search history, and time of day) and personal data (with user consent) to tailor search results more closely to the individual's intent and interests.\n",
      "\n",
      "5. **Multimodal Search**: Advancements in understanding and integrating different types of data (text, images, voice, video) in search queries. For example, Google's Multitask Unified Model (MUM) is designed to understand information across languages and formats, enabling users to ask complex questions and receive comprehensive answers that incorporate insights from various sources.\n",
      "\n",
      "6. **Voice Search and Conversational AI**: With the proliferation of smart speakers and voice-activated assistants, there's been a push to improve the ability of AI to engage in more natural, conversational interactions. This includes better understanding of spoken language, intent recognition, and providing more natural responses.\n",
      "\n",
      "7. **AI and Machine Learning for Spam Detection**: Enhanced use of AI to identify and filter out low-quality content, ensuring that users are presented with the most relevant, high-quality results.\n",
      "\n",
      "These advancements are part of an ongoing effort to make search engines more intuitive, efficient, and capable of understanding and responding to the complexities of human language and intent.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye bye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    q = input('Your question: ')\n",
    "    if q.lower() in 'exit quit bye':\n",
    "        print('Bye bye!')\n",
    "        break\n",
    "    result = ask_question(q, crc)\n",
    "    print(result['answer'])\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0964e8-515a-4cef-8dce-9be419991ff5",
   "metadata": {},
   "source": [
    "## Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f92f6744-1f09-491d-b546-72ac6002e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "retriever = chroma_vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "Before answering translate your response.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''Question: ```{question}```'''\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt },\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d678a555-4fcf-4e58-ad07-4f150cb10941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nUse the following pieces of context to answer the user\\'s question.\\nBefore answering translate your response.\\nIf you don\\'t find the answer in the provided context, just respond \"I don\\'t know.\"\\n---------------\\nContext: ```{context}```\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question: ```{question}```'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82f9b739-d813-49d5-b819-9b45f22e11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-```\n",
      "\n",
      "Human: Question: ```How many pairs of questions and answers had the StackOverflow dataset?```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?', additional_kwargs={}, response_metadata={}), AIMessage(content='Pregunta: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```', additional_kwargs={}, response_metadata={})], 'answer': 'Pregunta: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bda4cb19-f994-40ad-9312-b4140e0fc272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: Pregunta: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```\n",
      "Follow Up Input: When was Elon Musk born?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```Follow us\n",
      "Telecommunications\n",
      "By Ankur Jain • 7-minute read\n",
      "Gaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute read\n",
      "MWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow us\n",
      "Telecommunications\n",
      "By Ankur Jain • 7-minute read\n",
      "Gaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute read\n",
      "MWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow us\n",
      "Telecommunications\n",
      "By Ankur Jain • 7-minute read\n",
      "Gaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute read\n",
      "MWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow us\n",
      "Telecommunications\n",
      "By Ankur Jain • 7-minute read\n",
      "Gaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute read\n",
      "MWC’24: Unlocking the AI-enabled Telco with\n",
      "\n",
      "Follow us\n",
      "Telecommunications\n",
      "By Ankur Jain • 7-minute read\n",
      "Gaming\n",
      "By Patrick Smith • 8-minute read\n",
      "Application Development\n",
      "By Mete Atamel • 5-minute read\n",
      "AI & Machine Learning\n",
      "By Burak Gokturk • 3-minute read\n",
      "MWC’24: Unlocking the AI-enabled Telco with```\n",
      "\n",
      "Human: Question: ```¿Cuándo nació Elon Musk?```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'When was Elon Musk born?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?', additional_kwargs={}, response_metadata={}), AIMessage(content='Pregunta: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```', additional_kwargs={}, response_metadata={}), HumanMessage(content='When was Elon Musk born?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={})], 'answer': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'When was Elon Musk born?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c84f5229-bf59-46ec-b287-cde1e29871ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: Pregunta: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```\n",
      "Human: When was Elon Musk born?\n",
      "Assistant: I don't know.\n",
      "Follow Up Input: When was Bill Gates born?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Use the following pieces of context to answer the user's question.\n",
      "Before answering translate your response.\n",
      "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
      "---------------\n",
      "Context: ```Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog\n",
      "\n",
      "Google Cloud Google Cloud Products Privacy Terms\n",
      "Help  English \n",
      "Contact sales Get started for freeCloud Blog```\n",
      "\n",
      "Human: Question: ```¿Cuándo nació Bill Gates?```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'When was Bill Gates born?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?', additional_kwargs={}, response_metadata={}), AIMessage(content='Pregunta: ```¿Cuántos pares de preguntas y respuestas tenía el conjunto de datos de StackOverflow?```', additional_kwargs={}, response_metadata={}), HumanMessage(content='When was Elon Musk born?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='When was Bill Gates born?', additional_kwargs={}, response_metadata={}), AIMessage(content='No sé.', additional_kwargs={}, response_metadata={})], 'answer': 'No sé.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'When was Bill Gates born?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad93d010-5395-466b-a196-975ab1ca1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sé.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d62d7-3de4-40d8-bf00-bdedca083c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
